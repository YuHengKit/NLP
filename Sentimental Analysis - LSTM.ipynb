{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import re\n",
    "import codecs\n",
    "import os\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "#%%\n",
    "################################### Paths to Data ########################################################################\n",
    "\n",
    "path = '/Data/'\n",
    "gloveFile = '/Data/glove/glove_6B_300d.txt' #'/Users/prajwalshreyas/Desktop/Singularity/Topic modelling/Glove/glove.twitter.27B/glove.twitter.27B.25d.txt'\n",
    "vocab_path = '/Data/glove/vocab_glove.csv'\n",
    "\n",
    "#Split Data path\n",
    "train_data_path ='/Data/TrainingData/train.csv'\n",
    "val_data_path ='/Data/TrainingData/val.csv'\n",
    "test_data_path ='/Data/TrainingData/test.csv'\n",
    "\n",
    "sent_matrix_path ='/Data/inputs_model/sentence_matrix.csv'\n",
    "sent_matrix_path_val ='/Data/inputs_model/sentence_matrix_val.csv'\n",
    "sent_matrix_path_test ='/Data/inputs_model/sentence_matrix_test.csv'\n",
    "sequence_len_path = '/Data/inputs_model/sequence_length.csv'\n",
    "sequence_len_val_path = '/Data/inputs_model/sequence_length_val.csv'\n",
    "sequence_len_test_path = '/Data/inputs_model/sequence_length_test.csv'\n",
    "wordVectors_path = '/Data/inputs_model/wordVectors.csv'\n",
    "#%%#\n",
    "\n",
    "#<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<< Filtered Vocabulary from Glove document >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
    "def filter_glove(full_glove_path, data_dir):\n",
    "  vocab = set()\n",
    "  sentence_path = os.path.join(data_dir,'SOStr.txt')\n",
    "  filtered_glove_path = os.path.join(data_dir, 'filtered_glove.txt')\n",
    "  # Download the full set of unlabeled sentences separated by '|'.\n",
    "  #sentence_path, = download_and_unzip(\n",
    "    #'http://nlp.stanford.edu/~socherr/', 'stanfordSentimentTreebank.zip',\n",
    "    #'stanfordSentimentTreebank/SOStr.txt')\n",
    "  with codecs.open(sentence_path, encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "      # Drop the trailing newline and strip backslashes. Split into words.\n",
    "      vocab.update(line.strip().replace('\\\\', '').split('|'))\n",
    "  nread = 0\n",
    "  nwrote = 0\n",
    "  with codecs.open(full_glove_path, encoding='utf-8') as f:\n",
    "    with codecs.open(filtered_glove_path, 'w', encoding='utf-8') as out:\n",
    "      for line in f:\n",
    "        nread += 1\n",
    "        line = line.strip()\n",
    "        if not line: continue\n",
    "        if line.split(u' ', 1)[0] in vocab:\n",
    "          out.write(line + '\\n')\n",
    "          nwrote += 1\n",
    "  print('read %s lines, wrote %s' % (nread, nwrote))\n",
    "#%%#\n",
    "\n",
    "#<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<< Filtered Vocabulary from live cases >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
    "\n",
    "\n",
    "\n",
    "# <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<< load embeddings >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
    "\n",
    "def load_embeddings(embedding_path):\n",
    "  \"\"\"Loads embedings, returns weight matrix and dict from words to indices.\"\"\"\n",
    "  print('loading word embeddings from %s' % embedding_path)\n",
    "  weight_vectors = []\n",
    "  word_idx = {}\n",
    "  with codecs.open(embedding_path, encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "      word, vec = line.split(u' ', 1)\n",
    "      word_idx[word] = len(weight_vectors)\n",
    "      weight_vectors.append(np.array(vec.split(), dtype=np.float32))\n",
    "  # Annoying implementation detail; '(' and ')' are replaced by '-LRB-' and\n",
    "  # '-RRB-' respectively in the parse-trees.\n",
    "  word_idx[u'-LRB-'] = word_idx.pop(u'(')\n",
    "  word_idx[u'-RRB-'] = word_idx.pop(u')')\n",
    "  # Random embedding vector for unknown words.\n",
    "  weight_vectors.append(np.random.uniform(\n",
    "      -0.05, 0.05, weight_vectors[0].shape).astype(np.float32))\n",
    "  return np.stack(weight_vectors), word_idx\n",
    "\n",
    "\n",
    "# Combine and split the data into train and test\n",
    "def read_data(path):\n",
    "    # read dictionary into df\n",
    "    df_data_sentence = pd.read_table(path + 'dictionary.txt')\n",
    "    df_data_sentence_processed = df_data_sentence['Phrase|Index'].str.split('|', expand=True)\n",
    "    df_data_sentence_processed = df_data_sentence_processed.rename(columns={0: 'Phrase', 1: 'phrase_ids'})\n",
    "\n",
    "    # read sentiment labels into df\n",
    "    df_data_sentiment = pd.read_table(path + 'sentiment_labels.txt')\n",
    "    df_data_sentiment_processed = df_data_sentiment['phrase ids|sentiment values'].str.split('|', expand=True)\n",
    "    df_data_sentiment_processed = df_data_sentiment_processed.rename(columns={0: 'phrase_ids', 1: 'sentiment_values'})\n",
    "\n",
    "\n",
    "    #combine data frames containing sentence and sentiment\n",
    "    df_processed_all = df_data_sentence_processed.merge(df_data_sentiment_processed, how='inner', on='phrase_ids')\n",
    "\n",
    "    return df_processed_all\n",
    "\n",
    "def training_data_split(all_data, spitPercent, data_dir):\n",
    "\n",
    "    msk = np.random.rand(len(all_data)) < spitPercent\n",
    "    train_only = all_data[msk]\n",
    "    test_and_dev = all_data[~msk]\n",
    "\n",
    "\n",
    "    msk_test = np.random.rand(len(test_and_dev)) <0.5\n",
    "    test_only = test_and_dev[msk_test]\n",
    "    dev_only = test_and_dev[~msk_test]\n",
    "\n",
    "    dev_only.to_csv(os.path.join(data_dir, 'TrainingData/dev.csv'))\n",
    "    test_only.to_csv(os.path.join(data_dir, 'TrainingData/test.csv'))\n",
    "    train_only.to_csv(os.path.join(data_dir, 'TrainingData/train.csv'))\n",
    "\n",
    "    return train_only, test_only, dev_only\n",
    "#%%\n",
    "################################### Glove Vector  ########################################################################\n",
    "def loadGloveModel(gloveFile):\n",
    "    print (\"Loading Glove Model\")\n",
    "    f = open(gloveFile,'r',encoding='utf-8')\n",
    "    model = {}\n",
    "    for line in f:\n",
    "        try:\n",
    "            splitLine = line.split()\n",
    "            word = splitLine[0]\n",
    "            embedding = [float(val) for val in splitLine[1:]]\n",
    "            model[word] = embedding\n",
    "        except:\n",
    "            print (word)\n",
    "            continue\n",
    "\n",
    "    print (\"Done.\",len(model),\" words loaded!\")\n",
    "    return model\n",
    "#%%\n",
    "\n",
    "\n",
    "#%%\n",
    "################################### Create Vocab subset GLove vectors ########################################################################\n",
    "\n",
    "def word_vec_index(training_data, glove_model):\n",
    "\n",
    "    sentences = training_data['Phrase'] # get the phrases as a df series\n",
    "    #sentences = sentences[0:100]\n",
    "    sentences_concat = sentences.str.cat(sep=' ')\n",
    "    sentence_words = re.findall(r'\\S+', sentences_concat)\n",
    "    sentence_words_lwr = [x.lower() for x in sentence_words]\n",
    "    subdict = {word: glove_model[word] for word in glove_model.keys() & sentence_words_lwr}\n",
    "\n",
    "    vocab_df = pd.DataFrame(subdict)\n",
    "    vocab_df.to_csv(vocab_path)\n",
    "    return vocab_df\n",
    "#%%\n",
    "################################### Convertdf to list ########################################################################\n",
    "def word_list(vocab_df):\n",
    "\n",
    "    wordVectors = vocab_df.values.T.tolist()\n",
    "    wordVectors_np = np.array(wordVectors)\n",
    "    wordList = list(vocab_df.columns.values)\n",
    "\n",
    "    return wordList, wordVectors_np\n",
    " #%%\n",
    "################################### tensorflow data pipeline ########################################################################\n",
    "\n",
    "\n",
    "def maxSeqLen(training_data):\n",
    "\n",
    "    total_words = 0\n",
    "    sequence_length = []\n",
    "    idx = 0\n",
    "    for index, row in training_data.iterrows():\n",
    "\n",
    "        sentence = (row['Phrase'])\n",
    "        sentence_words = sentence.split(' ')\n",
    "        len_sentence_words = len(sentence_words)\n",
    "        total_words = total_words + len_sentence_words\n",
    "\n",
    "        # get the length of the sequence of each training data\n",
    "        sequence_length.append(len_sentence_words)\n",
    "\n",
    "        if idx == 0:\n",
    "            max_seq_len = len_sentence_words\n",
    "\n",
    "\n",
    "        if len_sentence_words > max_seq_len:\n",
    "            max_seq_len = len_sentence_words\n",
    "        idx = idx + 1\n",
    "\n",
    "    avg_words = total_words/index\n",
    "\n",
    "    # convert to numpy array\n",
    "    sequence_length_np = np.asarray(sequence_length)\n",
    "\n",
    "    return max_seq_len, avg_words, sequence_length_np\n",
    "\n",
    "  #%%\n",
    "def tf_data_pipeline(data, word_idx, weight_matrix, max_seq_len):\n",
    "\n",
    "    #training_data = training_data[0:50]\n",
    "\n",
    "    maxSeqLength = max_seq_len #Maximum length of sentence\n",
    "    no_rows = len(data)\n",
    "    ids = np.zeros((no_rows, maxSeqLength), dtype='int32')\n",
    "    # conver keys in dict to lower case\n",
    "    word_idx_lwr =  {k.lower(): v for k, v in word_idx.items()}\n",
    "    idx = 0\n",
    "\n",
    "    for index, row in data.iterrows():\n",
    "\n",
    "\n",
    "        sentence = (row['Phrase'])\n",
    "        sentence_words = sentence.split(' ')\n",
    "\n",
    "        i = 0\n",
    "        for word in sentence_words:\n",
    "            #print(index)\n",
    "            word_lwr = word.lower()\n",
    "            try:\n",
    "                #print (word_lwr)\n",
    "                ids[idx][i] =  word_idx_lwr[word_lwr]\n",
    "\n",
    "            except Exception as e:\n",
    "                #print (e)\n",
    "                #print (word)\n",
    "                if str(e) == word:\n",
    "                    ids[idx][i] = 0\n",
    "                continue\n",
    "            i = i + 1\n",
    "        idx = idx + 1\n",
    "    return ids\n",
    "\n",
    "  #%%\n",
    "# create labels matrix for the rnn\n",
    "\n",
    "\n",
    "def tf_data_pipeline_nltk(data, word_idx, weight_matrix, max_seq_len):\n",
    "\n",
    "    #training_data = training_data[0:50]\n",
    "\n",
    "    maxSeqLength = max_seq_len #Maximum length of sentence\n",
    "    no_rows = len(data)\n",
    "    ids = np.zeros((no_rows, maxSeqLength), dtype='int32')\n",
    "    # conver keys in dict to lower case\n",
    "    word_idx_lwr =  {k.lower(): v for k, v in word_idx.items()}\n",
    "    idx = 0\n",
    "\n",
    "    for index, row in data.iterrows():\n",
    "\n",
    "\n",
    "        sentence = (row['Phrase'])\n",
    "        #print (sentence)\n",
    "        tokenizer = RegexpTokenizer(r'\\w+')\n",
    "        sentence_words = tokenizer.tokenize(sentence)\n",
    "        #print (sentence_words)\n",
    "        i = 0\n",
    "        for word in sentence_words:\n",
    "            #print(index)\n",
    "            word_lwr = word.lower()\n",
    "            try:\n",
    "                #print (word_lwr)\n",
    "                ids[idx][i] =  word_idx_lwr[word_lwr]\n",
    "\n",
    "            except Exception as e:\n",
    "                #print (e)\n",
    "                #print (word)\n",
    "                if str(e) == word:\n",
    "                    ids[idx][i] = 0\n",
    "                continue\n",
    "            i = i + 1\n",
    "        idx = idx + 1\n",
    "\n",
    "    return ids\n",
    "\n",
    "\n",
    "def labels_matrix(data):\n",
    "\n",
    "    labels = data['sentiment_values']\n",
    "\n",
    "    lables_float = labels.astype(float)\n",
    "\n",
    "    cats = ['0','1','2','3','4','5','6','7','8','9']\n",
    "    labels_mult = (lables_float * 10).astype(int)\n",
    "    dummies = pd.get_dummies(labels_mult, prefix='', prefix_sep='')\n",
    "    dummies = dummies.T.reindex(cats).T.fillna(0)\n",
    "    labels_matrix = dummies.as_matrix()\n",
    "\n",
    "    return labels_matrix\n",
    "\n",
    "\n",
    "def labels_matrix_unmod(data):\n",
    "\n",
    "    labels = data['sentiment_values']\n",
    "\n",
    "    lables_float = labels.astype(float)\n",
    "\n",
    "    labels_mult = (lables_float * 10).astype(int)\n",
    "    labels_matrix = labels_mult.as_matrix()\n",
    "\n",
    "    return labels_matrix\n",
    "\n",
    "#%%\n",
    "################################### Run Steps ########################################################################\n",
    "def main():\n",
    "\n",
    "    # Load the Trainign data\n",
    "    all_data = read_data(path)\n",
    "    #%%\n",
    "    training_data = pd.read_csv(train_data_path, encoding='iso-8859-1')\n",
    "\n",
    "    # use the below to split the training, validation and test\n",
    "    train_df = training_data_split(training_data)\n",
    "    #%%\n",
    "\n",
    "    # Load glove vector\n",
    "    glove_model = filter_glove(gloveFile)\n",
    "\n",
    "    # Get glove vector subset for training vocab\n",
    "    vocab_df = word_vec_index(all_data, glove_model)\n",
    "    glove_model = None\n",
    "\n",
    "    #Run this after the first iteration of obtaining the vocab df instead of above 2 steps\n",
    "    vocab_df = pd.read_csv(vocab_path, encoding='iso-8859-1')\n",
    "\n",
    "    #Get Wordlist and word vec lists from the df for the training Vocab\n",
    "    wordList, wordVectors = word_list(vocab_df)\n",
    "    wordVectors_df = pd.DataFrame(wordVectors)\n",
    "    wordVectors_df.to_csv(wordVectors_path)\n",
    "\n",
    "    # get the index of the word vec for each sentences to be input to the tf algo\n",
    "    max_seq_len, avg_len, sequence_length = maxSeqLen(training_data)\n",
    "    sequence_length_df = pd.DataFrame(sequence_length)\n",
    "    sequence_length_df.to_csv(sequence_len_path)\n",
    "\n",
    "    # training data input matrix\n",
    "    sentence_matrix = tf_data_pipeline(training_data, wordList, wordVectors, max_seq_len)\n",
    "\n",
    "    # export the sentence matrix to a csv file for easy load for next iterations\n",
    "    sentence_matrix_df = pd.DataFrame(sentence_matrix)\n",
    "    sentence_matrix_df.to_csv(sent_matrix_path)\n",
    "\n",
    "    #################################################################### validation data set ############################################################\n",
    "    # load validation data\n",
    "    val_data = pd.read_csv(val_data_path, encoding='iso-8859-1')\n",
    "\n",
    "    # load glove model and generat vocab for validation data\n",
    "    glove_model = loadGloveModel(gloveFile)\n",
    "    vocab_df_val = word_vec_index(val_data, glove_model)\n",
    "    glove_model = None\n",
    "    wordList_val, wordVectors_val = word_list(vocab_df_val)\n",
    "\n",
    "    # get max length for val data\n",
    "    max_seq_len_val, avg_len_val, sequence_length_val = maxSeqLen(val_data)\n",
    "    sequence_length_val_df = pd.DataFrame(sequence_length_val)\n",
    "    sequence_length_val_df.to_csv(sequence_len_val_path)\n",
    "\n",
    "    # get the id matrix for val data\n",
    "    sentence_matrix_val = tf_data_pipeline(val_data, wordList_val, wordVectors_val, max_seq_len)\n",
    "\n",
    "    # write the val dat to csv\n",
    "    sentence_matrix_df_val = pd.DataFrame(sentence_matrix_val)\n",
    "    sentence_matrix_df_val.to_csv(sent_matrix_path_val)\n",
    "\n",
    "    #################################################################### Test data set ############################################################\n",
    "    # load test data\n",
    "    test_data = pd.read_csv(test_data_path, encoding='iso-8859-1')\n",
    "\n",
    "    # load glove model and generat vocab for test data\n",
    "    glove_model = loadGloveModel(gloveFile)\n",
    "    vocab_df_test = word_vec_index(val_data, glove_model)\n",
    "    glove_model = None\n",
    "    wordList_test, wordVectors_test = word_list(vocab_df_test)\n",
    "\n",
    "    # get max length for test data\n",
    "    max_seq_len_test, avg_len_test, sequence_length_test = maxSeqLen(test_data)\n",
    "    sequence_length_test_df = pd.DataFrame(sequence_length_test)\n",
    "    sequence_length_test_df.to_csv(sequence_len_test_path)\n",
    "\n",
    "    # get the id matrix for test data\n",
    "    sentence_matrix_test = tf_data_pipeline(test_data, wordList_test, wordVectors_test, max_seq_len_test)\n",
    "\n",
    "    # write the test dat to csv\n",
    "    sentence_matrix_df_test= pd.DataFrame(sentence_matrix_test)\n",
    "    sentence_matrix_df_test.to_csv(sent_matrix_path_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading word embeddings from Data/glove/glove_6B_100d.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Heng1222\\Anaconda3\\lib\\site-packages\\keras\\engine\\saving.py:384: UserWarning: Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n",
      "  warnings.warn('Error in loading the saved optimizer '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 56, 100)           40000100  \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 256)               234496    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               131584    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                5130      \n",
      "=================================================================\n",
      "Total params: 40,371,310\n",
      "Trainable params: 371,210\n",
      "Non-trainable params: 40,000,100\n",
      "_________________________________________________________________\n",
      "[  353    20    14 24150   373]\n",
      "0.34\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import LSTM\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers import Bidirectional\n",
    "from keras.preprocessing import sequence\n",
    "from keras.layers import Dropout\n",
    "import h5py\n",
    "from keras.models import model_from_json\n",
    "from keras.models import load_model\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "def load_data_all(data_dir, all_data_path,pred_path, gloveFile, first_run, load_all):\n",
    "\n",
    "    # Load embeddings for the filtered glove list\n",
    "    if load_all == True:\n",
    "        weight_matrix, word_idx = load_embeddings(gloveFile)\n",
    "    else:\n",
    "        weight_matrix, word_idx = load_embeddings(filtered_glove_path)\n",
    "\n",
    "    len(word_idx)\n",
    "    len(weight_matrix)\n",
    "\n",
    "    #%%\n",
    "    # create test, validation and trainng data\n",
    "    all_data = read_data(all_data_path)\n",
    "    train_data, test_data, dev_data = training_data_split(all_data, 0.8, data_dir)\n",
    "\n",
    "    train_data = train_data.reset_index()\n",
    "    dev_data = dev_data.reset_index()\n",
    "    test_data = test_data.reset_index()\n",
    "\n",
    "    #%%\n",
    "    # inputs from dl_sentiment that are hard coded but need to be automated\n",
    "    maxSeqLength, avg_words, sequence_length = maxSeqLen(all_data)\n",
    "    numClasses = 10\n",
    "    #%%\n",
    "\n",
    "     # load Training data matrix\n",
    "    train_x = tf_data_pipeline_nltk(train_data, word_idx, weight_matrix, maxSeqLength)\n",
    "    test_x = tf_data_pipeline_nltk(test_data, word_idx, weight_matrix, maxSeqLength)\n",
    "    val_x = tf_data_pipeline_nltk(dev_data, word_idx, weight_matrix, maxSeqLength)\n",
    "\n",
    "    #%%\n",
    "    # load labels data matrix\n",
    "    train_y = labels_matrix(train_data)\n",
    "    val_y = labels_matrix(dev_data)\n",
    "    test_y = labels_matrix(test_data)\n",
    "\n",
    "\n",
    "     #%%\n",
    "\n",
    "    # summarize size\n",
    "    print(\"Training data: \")\n",
    "    print(train_x.shape)\n",
    "    print(train_y.shape)\n",
    "\n",
    "    # Summarize number of classes\n",
    "    print(\"Classes: \")\n",
    "    print(np.unique(train_y.shape[1]))\n",
    "\n",
    "    return train_x, train_y, test_x, test_y, val_x, val_y, weight_matrix, word_idx\n",
    "\n",
    "def create_model_rnn(weight_matrix, max_words, EMBEDDING_DIM):\n",
    "\n",
    "    # create the model\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(len(weight_matrix), EMBEDDING_DIM, weights=[weight_matrix], input_length=max_words, trainable=False))\n",
    "    model.add(Bidirectional(LSTM(128, dropout=0.2, recurrent_dropout=0.2)))\n",
    "    model.add(Dense(512, activation='relu'))\n",
    "    model.add(Dropout(0.50))\n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "    # try using different optimizers and different optimizer configs\n",
    "    model.compile(loss='categorical_crossentropy',optimizer='adam', metrics=['accuracy'])\n",
    "    print(model.summary())\n",
    "\n",
    "    return model\n",
    "\n",
    "def train_model(model,train_x, train_y, test_x, test_y, val_x, val_y, batch_size, path) :\n",
    "\n",
    "    # save the best model and early stopping\n",
    "    saveBestModel = keras.callbacks.ModelCheckpoint(path+'/model/best_model.hdf5', monitor='val_acc', verbose=0, save_best_only=True, save_weights_only=False, mode='auto', period=1)\n",
    "    earlyStopping = keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=3, verbose=0, mode='auto')\n",
    "\n",
    "    # Fit the model\n",
    "    model.fit(train_x, train_y, batch_size=batch_size, epochs=25,validation_data=(val_x, val_y), callbacks=[saveBestModel, earlyStopping])\n",
    "    # Final evaluation of the model\n",
    "    score, acc = model.evaluate(test_x, test_y, batch_size=batch_size)\n",
    "\n",
    "    print('Test score:', score)\n",
    "    print('Test accuracy:', acc)\n",
    "\n",
    "    return model\n",
    "\n",
    "def live_test(trained_model, data, word_idx):\n",
    "\n",
    "    #data = \"Pass the salt\"\n",
    "    #data_sample_list = data.split()\n",
    "    live_list = []\n",
    "    live_list_np = np.zeros((56,1))\n",
    "    # split the sentence into its words and remove any punctuations.\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    data_sample_list = tokenizer.tokenize(data)\n",
    "\n",
    "    labels = np.array(['1','2','3','4','5','6','7','8','9','10'], dtype = \"int\")\n",
    "    #word_idx['I']\n",
    "    # get index for the live stage\n",
    "    data_index = np.array([word_idx[word.lower()] if word.lower() in word_idx else 0 for word in data_sample_list])\n",
    "    data_index_np = np.array(data_index)\n",
    "    print(data_index_np)\n",
    "\n",
    "    # padded with zeros of length 56 i.e maximum length\n",
    "    padded_array = np.zeros(56) # use the def maxSeqLen(training_data) function to detemine the padding length for your data\n",
    "    padded_array[:data_index_np.shape[0]] = data_index_np\n",
    "    data_index_np_pad = padded_array.astype(int)\n",
    "    live_list.append(data_index_np_pad)\n",
    "    live_list_np = np.asarray(live_list)\n",
    "    type(live_list_np)\n",
    "\n",
    "    # get score from the model\n",
    "    score = trained_model.predict(live_list_np, batch_size=1, verbose=0)\n",
    "    #print (score)\n",
    "\n",
    "    single_score = np.round(np.argmax(score)/10, decimals=2) # maximum of the array i.e single band\n",
    "\n",
    "    # weighted score of top 3 bands\n",
    "    top_3_index = np.argsort(score)[0][-3:]\n",
    "    top_3_scores = score[0][top_3_index]\n",
    "    top_3_weights = top_3_scores/np.sum(top_3_scores)\n",
    "    single_score_dot = np.round(np.dot(top_3_index, top_3_weights)/10, decimals = 2)\n",
    "\n",
    "    #print (single_score)\n",
    "    return single_score_dot\n",
    "\n",
    "def main():\n",
    "\n",
    "    max_words = 56 # max no of words in your training data\n",
    "    batch_size = 2000 # batch size for training\n",
    "    EMBEDDING_DIM = 100 # size of the word embeddings\n",
    "    train_flag = False # set True if in training mode else False if in prediction mode\n",
    "\n",
    "    if train_flag:\n",
    "        # create training, validataion and test data sets\n",
    "        # load the dataset\n",
    "        \n",
    "        data_dir = 'Data'\n",
    "        all_data_path = 'Data/'\n",
    "        pred_path = 'Data/output_model/test_pred.csv'\n",
    "        gloveFile = 'Data/glove/glove_6B_100d.txt'\n",
    "        first_run = False\n",
    "        load_all = True\n",
    "\n",
    "        train_x, train_y, test_x, test_y, val_x, val_y, weight_matrix, word_idx = load_data_all(data_dir, all_data_path,pred_path, gloveFile, first_run, load_all)\n",
    "        # create model strucutre\n",
    "        model = create_model_rnn(weight_matrix, max_words, EMBEDDING_DIM)\n",
    "\n",
    "        # train the model\n",
    "        trained_model =train_model(model,train_x, train_y, test_x, test_y, val_x, val_y, batch_size, path)   # run model live\n",
    "\n",
    "\n",
    "        # serialize weights to HDF5\n",
    "        model.save_weights(path+\"/model/best_model.h5\")\n",
    "        print(\"Saved model to disk\")\n",
    "\n",
    "    else:\n",
    "        gloveFile = 'Data/glove/glove_6B_100d.txt'\n",
    "        weight_matrix, word_idx = load_embeddings(gloveFile)\n",
    "        weight_path = 'best_model.hdf5'\n",
    "        loaded_model = load_model(weight_path)\n",
    "        loaded_model.summary()\n",
    "        data_sample = \"Great!! it is raining today!!\"\n",
    "        result = live_test(loaded_model,data_sample, word_idx)\n",
    "        print (result)\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
